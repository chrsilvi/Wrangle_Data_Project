{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling We Rate Dogs Data #\n",
    "\n",
    "A lot of the wrangling was about coming to understand the internal logic of how the account works. There is actually a very coherent system to the ratings: all dogs are out of 10, with ratings between 10 and 14, and although a 15 is not impossible, it has not occurred yet. This information extrapolates out into some strange data at first glance; for example, if there are multiple dogs in a photo, the rating scale adds 10 for each dog, so a rating of a group of nine dogs would be out of 90. Exceptions to the scale are infrequently made for jokes, such as one rating of 1776 for a US patriotic dog, or one case of 9.75 for a Harry Potter dressed dog, where the number itself has some other value and isn't actually a specific rating on the dog. Once I understood that, it was easier to understand what data fit and what data needed adjusting in order to match.  \n",
    "\n",
    "I think the most challenging part is dealing with the names and the dog stages. The names are typically introduced in similar fashions, such as \"This isâ€¦\", but then depending on those formulaic phrases leads to errors when a sentence without the dog's name starts in a similar way. The account also frequently uses puns and multiple references of the names of the dog stages within a single tweet, making it hard to distinguish how the dogs are classified. What complicates that further is that there is not a concrete delineation of what each stage means, so even if a person were to go through manually and try to ascertain the dog stage, there would be no completely accurate data. That gets complicated further by many of the posts having multiple dogs.\n",
    "\n",
    "What eliminated a lot of the messiness and work in the data was that we wanted to work only with posts that had favorite and retweet counts, and after running the API, only about 37% of the tweets in the original archive successfully pulled that data, which was a lot smaller data set to work with. Going through the full original set of data would be much more difficult, and as it was my final product was not entirely cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
